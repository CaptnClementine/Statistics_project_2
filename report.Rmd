---
title: "r_project_2"
author: "Michil Trofimov, Anastasia Gorabarenko"
date: "2023-11-21"
output: html_document
---

# Libraries

```{r library, message=FALSE}
#if you don' have these new libraries, please uncomment (Shift+Ctrl+C) next 3 strings:
# install.packages("data.table")
# install.packages("dlookr")
# install.packages("flextable")
library(data.table) # for faster data upload
library(ggplot2)
library(dplyr)
library(car)
library(performance)
library(dlookr) # for easier EDA
library(flextable) # for more beautiful tables
dplyr::filter
```

```{min r, include=FALSE}
main_dir <- dirname(rstudioapi::getSourceEditorContext()$path) 
setwd(main_dir)
```


# Dataset description

Данный датасет содержит информацию о полевых испытаниях ~300 сортов (id) сои в 2 локациях в течение 5 лет. Измерялись productivity (г/м^2), oil_content, protein_content (в процентах) и vegetation_period (время в днях от всхода до сбора). Кроме того известны форма листьев (leaf_shape), группа созревания (maturation_group, чем больше тем более позднеспелый), группа цветения (flowering_group), цвет опушения (pubescence_colour), цвет венчика (corolla_colour), страна происхождения (origin), полегание (lodging_type), тип роста (growth_type, индетерминантный - цветёт до сбора, детерминантный - цветёт один раз). 

2943 observations and 16 columns.

# Exploratory Data Analysis

## Analyse NA observations and outliers

```{r upload data, include=FALSE}
df = fread('soybean.csv', key='id', na.strings = c('',NA))
df = df[,-1]
```

```{r, echo=FALSE}
diagnose(df) %>% flextable()
```

We see that almost a half of observations are missing in numerical variables. Other than that, there are 108 missing observations in `origin` features.

First, we need to factorize id, leaf_shape, maturation_group, lodging_type, growth_type, flowering_group, pubescence_colour, corolla_colour, origin, site, year. 

Second, check missing observations in `origin`.

Third, remove missing observations in numerical features.

Fourth, analyse outliers.

1. Factorization 

```{r}
df = df %>%
  mutate(across(c('id', 'leaf_shape', 'maturation_group', 'lodging_type', 'growth_type', 'flowering_group', 'pubescence_colour', 'corolla_colour', 'origin', 'site', 'year'), as.factor))
```

2. NA in `origin`

Check missing observations in `origin`

```{r, echo=FALSE}
df[!complete.cases(df$origin)] %>% head %>% flextable() %>% autofit
```

We can notice that some observations have NA in `origin` column but not in numerical columns. It is pretty crucial, that is why we keep them. Thus, drop observations with NAs in `origin` and numerical columns

```{r, include=FALSE}
na_origin = df[!complete.cases(df$origin)]
na_origin_num_columns =  na_origin[!complete.cases(na_origin$productivity)]
v1_na_origin_num_columns = na_origin_num_columns$V1
df = df[!rownames(df) %in% v1_na_origin_num_columns]
```

3. NA in numerical columns

It would be more conservative to drop NA observations. We can impute them with mean of a breed (`id`), but some breeds have a lot of missing observations e.g. `id` = 1 has 7/9 NA in `productivity` column. And it is not crucial to fill NA, if a breed has very similar values in numerical column, for example: `id` = 8 6/8 NA in `productivity` but remaining two values are 51 and 53, mean = 52.

Due to that, let's drop missing values by `productivity` column.

```{r, include=FALSE}
df = df[complete.cases(df$productivity)]
```


4. Analysis of outliers

Check general descriptive statistics of numerical columns

```{r, echo=FALSE}
diagnose_numeric(df) %>% flextable()
```

Visualise numerical columns

```{r, echo=FALSE}
num_columns <- select(df, productivity, vegetation_period, protein_content, oil_content)
plot(num_columns)
```

How outliers affect our variables? 

```{r, echo=FALSE}
diagnose_outlier(df) %>%
  filter(outliers_cnt > 0) %>% flextable()
```

In all three variables outliers almost do not affect mean. These can be explained by the fact that outliers come from different breeds of plants, for example: extremely low producing breeds will generally give low `productivity` numbers.

## Feature analysis

```{r,echo=FALSE}
df %>% 
  correlate() %>% 
  plot()
```



```{r}
diagnose_category(df) %>% flextable()
```

# Anova analysis
## First ANOVA
```{r Anova1_plot, plotfig.align="center", fig.height=5, fig.width=7, echo=FALSE}
ggplot(df, aes(x = flowering_group, y = productivity)) +
  geom_boxplot() +
  labs(title = "Boxplot of Productivity by Flowering Group",
       x = "Flowering Group",
       y = "Productivity")
ggplot(df, aes(x = flowering_group, y = productivity, fill = flowering_group)) +
  geom_violin() +
  labs(title = "Violin Plot of Productivity by Flowering Group",
       x = "Flowering Group",
       y = "Productivity") +
  theme_minimal()
```

The distributions do not appear to be normal. To confirm this, re-evaluate using the Shapiro test. If the 'is_normal' column contains 'Normal,' it implies that we cannot reject the hypothesis that the data follows a normal distribution
```{r shapiro }
shapiro_test_result <- df %>%
  group_by(flowering_group) %>%
  summarise(p_value = shapiro.test(productivity)$p.value)
shapiro_test_result$is_normal <- ifelse(shapiro_test_result$p_value < 0.05, "Not Normal", "Normal")

# Вывод результатов теста
print(shapiro_test_result)

```
The observations are independent, therefore have (mostly) the normal distribution. In this case we can use ANOVA for this data. Let's do it!
```{r Anova }
anova_result <- aov(productivity ~ flowering_group, data = df)
check_normality(anova_result)

check_homogeneity(anova_result)
print('Anova result')
summary(anova_result)
```
The ANOVA results show a significant difference among at least two of the groups in the variable 'productivity' based on the 'flowering_group' factor.
The p-value (Pr(>F)) is highly significant (less than 0.001), indicating that there are significant differences among the groups.

But residuals have not  normal distribution, in this case we will re-check our data with the Kruskal-Wallis test (It is a non-parametric alternative to one-way ANOVA that doesn't assume normality of the residuals)
```{r tukey }
kruskal_test_result <- kruskal.test(productivity ~ flowering_group, data = df)
print(kruskal_test_result)
```
The p-value is extremely low (p-value < 2.2e-16), providing strong evidence to reject the null hypothesis. There are significant differences in the medians of productivity among the different groups defined by the variable flowering_group.

 Since the Kruskal-Wallis test doesn't tell us which specific groups are different, check it via post hoc bonferroni test.
**Post-Hoc Test:**
```{r tukey , warning=FALSE}
pairwise.wilcox.test(df$productivity, df$flowering_group, p.adjust.method = "bonferroni")

```
From the output of the Wilcoxon rank sum test with Bonferron correction, it follows that there are statistically significant differences in performance (productivity) between the different flowering group variants. The p-values are below the 0.05 innovativeness level for most pairs of groups.

Specifically, p-values < 0.05 indicate that there is a statistically significant difference in performance between findings 2 and 4, 3 and 4, 3.5 and 4.5, and 4 and 5. p-values <0.001 indicates very high values. statistical originality for these images.

Thus, it is possible to disable that the blooming_group group level affects productivity, and individual pairs of groups have characteristic significant differences.